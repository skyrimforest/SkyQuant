

主要采用**后训练量化（PTQ）**方式


| 方法              | 简述                                    | 常用于哪些模型           |
| --------------- | ------------------------------------- | ----------------- |
| 🤏 8-bit 量化     | 使用 `bitsandbytes` 加速器                 | BERT, LLaMA, Qwen |
| ⚙️ 4-bit 量化     | GPTQ、AWQ 等稀疏/低比特压缩方法                  | LLaMA, OPT, Qwen  |
| 🔧 动态量化         | 使用 PyTorch 的 `torch.quantize_dynamic` | BERT 等经典模型        |
| 🧪 QLoRA / PEFT | 低秩训练结合 4bit 权重量化                      | LLaMA 等大模型微调场景    |



| 任务                | 推荐方案                                  | 是否支持保存     |
| ----------------- | ------------------------------------- | ---------- |
| 快速加载 8bit/4bit 模型 | `transformers` + `bitsandbytes`       | ✅          |
| GPTQ 精度更高但更复杂     | `auto-gptq` 工具链                       | ✅          |
| 动态量化（仅 Linear）    | `torch.quantization.quantize_dynamic` | ✅          |
| 微调场景（QLoRA）       | `peft`, `qlora`, `bnb`                | ✅（lora 权重） |


常见的大模型压缩算法是GPTQ，其针对的是transformer架构，其前身为OBC。

Optimal Brain Compression（OBC，最优脑压缩），它是一个用于深度神经网络（DNNs）模型压缩的统一框架，专注于训练后（post-training）场景，即在不重新训练的情况下，仅通过少量校准数据实现模型的剪枝（pruning）与量化（quantization），同时保持较高精度。

针对训练后压缩场景，给定一个训练好的未压缩模型和少量校准数据，在不进行重新训练的前提下，通过单步压缩（one-shot）生成高精度的压缩模型，同时控制计算成本。该框架同时支持剪枝（移除权重）和量化（降低权重 / 激活精度）。

核心思路是将全局压缩任务分解为逐层（layer-wise）子问题，并基于经典的 Optimal Brain Surgeon（OBS）框架进行优化。

**逐层压缩问题定义** 对于每一层l，给定权重wl和输入xl，目标是找到压缩后的权重Wl，使得原始输出与压缩输出的平方误差最小，同时满足压缩约束（如稀疏度、量化精度）。

OBC 借鉴了 OBS 框架的思想（通过二阶信息选择待移除的权重），但针对现代 DNN 的规模进行了效率优化。
OBC 包含两个核心组件，分别对应剪枝和量化：

ExactOBS（剪枝组件）
用于实现高精度剪枝，核心是逐权重迭代剪枝：每次选择对输出误差影响最小的权重移除，并通过 Hessian 矩阵逆的高效更新（基于高斯消元）调整剩余权重，确保误差最小化。

OBQ（Optimal Brain Quantizer，量化组件）
用于实现高精度量化，核心是逐权重迭代量化：每次选择对损失增加影响最小的权重进行量化，并通过闭式更新调整剩余未量化权重，进一步降低误差。与剪枝类似，其计算复杂度经过优化，可高效处理大规模模型。