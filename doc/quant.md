

量化模块的输入：

| 输入项                                  | 类型                        | 描述                         |
| ------------------------------------ | ------------------------- | -------------------------- |
| ✅ 模型（全精度）                            | PyTorch 模型对象或路径（.pt / .bin） | 模型必须未被量化                   |
| ✅ 量化配置文件                             | `.yaml`    | 包含策略，如 bit-width、量化类型、模块范围 |
| ✅ 校准数据                               | DataLoader（可选）            | 用于收集激活分布（后训练量化）            |
| ❓ tokenizer / tokenizer config（Qwen） | dict / AutoTokenizer      | 仅 NLP 模型需要                 |


量化模块的输出：

| 输出项         | 类型                       | 描述                 |
| ----------- | ------------------------ | ------------------ |
| ✅ 量化模型      | `.pt` / `.bin` / `.gguf` | 可直接保存/加载用于部署       |
| ✅ 量化模型配置    | `.json` 或结构体             | 用于记录压缩信息（如模块策略）    |
| ✅ 可选推理接口包装器 | 类或脚本                     | 便于统一推理 API（支持原始接口） |


如何使用：

| 模型类型          | 工具链                | 是否能直接用原 API | 说明                             |
| ------------- | ------------------ | ----------- | ------------------------------ |
| ✅ ResNet（CNN） | PyTorch FX/PTQ/QAT | ✅ 是         | 可以 `model(input)`              |
| ✅ Qwen（LLM）   | GPTQ / AWQ         | ❌ 不行，需专用加载器 | 需使用 GPTQ/AWQ 推理代码或导入 llama.cpp |

🔹 ResNet 的量化模型是兼容 PyTorch 原生调用的（尤其是 QAT/PTQ）

🔹 Qwen 等大模型使用 GPTQ/AWQ 后，结构会被替换为 custom linear（不可直接加载）



有个问题就是如果对工具链描述的太详细，对用户本身的要求较高，用户应该只会从一些
场景本身的角度出发进行配置。因此总结出用户配置，其他的相关配置可以进行默认的设置。
也就是说，分为'用户的默认配置'与'工具链进阶配置'这两类。

也就是，对用户层维护一个统一的接口，用户使用本工具可以无视底层差异进行统一的配置；
而用户配置与底层工具链的交互、工具链与硬件平台的交互则由本系统默认完成。



ResNet使用
